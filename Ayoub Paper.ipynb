{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0923cf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 14 10:16:33 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.118.02   Driver Version: 440.118.02   CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN X (Pascal)    Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "| 67%   83C    P2    69W / 250W |  11595MiB / 12194MiB |      5%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN X (Pascal)    Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 61%   84C    P2   117W / 250W |   5612MiB / 12196MiB |     60%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN Xp            Off  | 00000000:81:00.0 Off |                  N/A |\n",
      "| 57%   84C    P2   160W / 250W |   5660MiB / 12196MiB |     63%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN Xp            Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 55%   84C    P2   211W / 250W |   8249MiB / 12196MiB |     80%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     11760      C   python                                     11585MiB |\n",
      "|    1      9473      C   python                                      2779MiB |\n",
      "|    1     15100      C   python                                      2823MiB |\n",
      "|    2     20758      C   python                                      2817MiB |\n",
      "|    2     22182      C   python                                      2833MiB |\n",
      "|    3      1827      C   python                                      2579MiB |\n",
      "|    3     17949      C   python                                      2827MiB |\n",
      "|    3     19776      C   python                                      2833MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "fatal: le chemin de destination 'morpholayers' existe déjà et n'est pas un répertoire vide.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!git clone https://github.com/Jacobiano/morpholayers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b5d21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "It should be >= 2.0.0.\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print('It should be >= 2.0.0.')\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[1], 'GPU')\n",
    "logical_devices = tf.config.list_logical_devices('GPU')\n",
    "print(logical_devices)\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.python.keras import regularizers\n",
    "import morpholayers.layers as ml\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist, mnist, cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca99dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class h_extrema_denoising_block_test_loss:\n",
    "    def __init__(self, rmax_reg_coeff = 1., hext_coeff = 1., \n",
    "                    train_shift=0.):\n",
    "        \n",
    "        self.hext_coeff = hext_coeff\n",
    "        self.rmax_reg_coeff = rmax_reg_coeff\n",
    "        self.train_shift = train_shift\n",
    "\n",
    "    def h_extrema_denoising_loss(self, y_true, y_pred):\n",
    "        #squared_difference = tf.square(y_true - y_pred)\n",
    "        #return tf.reduce_mean(squared_difference)\n",
    "        y_pred = tf.cast(y_pred, tf.double)\n",
    "        y_true = tf.cast(y_true, tf.double)\n",
    "        #print('h_extrema_denoising_loss : y_pred.shape',y_pred.shape)\n",
    "        #print('h_extrema_denoising_loss : y_true.shape',y_true.shape)\n",
    "        return self.hext_coeff * tf.reduce_mean(tf.math.squared_difference(y_true, y_pred)) #+self.train_shift))\n",
    "        \n",
    "    def rmax_reg_loss(self, y_true, y_pred):\n",
    "        #print('rmax_reg_loss : y_pred.shape',y_pred.shape)\n",
    "        #print('rmax_reg_loss : y_pred.shape',y_true.shape)\n",
    "        y_pred = tf.cast(y_pred, tf.double)\n",
    "        y = tf.cast(y_true, tf.double)\n",
    "        y_temp = y\n",
    "        #y = -tf.keras.layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(-y)\n",
    "        #y = tf.keras.layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(y)\n",
    "        y = kl.Lambda(ml.region_maxima_transform)(y)\n",
    "        y = kl.Multiply(name=\"r_max_reg\")([y_temp, 255.*y])\n",
    "        y = tf.cast(y, tf.double)\n",
    "        \n",
    "        return self.rmax_reg_coeff * tf.reduce_mean(tf.math.squared_difference(y_pred,y))\n",
    "\n",
    "    def __call__(self):\n",
    "        return {\"mse_denoising\":  self.h_extrema_denoising_loss\n",
    "                ,\"r_max_reg\": self.rmax_reg_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37fb1639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMNIST():\n",
    "    def __init__(self, mode=\"classification\", noise_train={\"mean\":0, \"std\":[0]}, \n",
    "                noise_val={\"mean\":0, \"std\":[0]}, reconstruction=True,\n",
    "                noise=None, shuffle=True, name=\"\",  X=None, X_noised=None, y=None,\n",
    "                buffer_size=10000):\n",
    "        self.noise = noise\n",
    "        self.name = name\n",
    "        self.noise_train = noise_train\n",
    "        self.noise_val = noise_val\n",
    "        self.mode = mode\n",
    "        self.X = X\n",
    "        self.X_noised = X_noised\n",
    "        self.y = y\n",
    "        self.rec = reconstruction\n",
    "\n",
    "\n",
    "    def create_dataset_from_data(self, X=None, X_noised=None, y=None):\n",
    "        if self.mode == \"classification\":\n",
    "            return tf.data.Dataset.from_tensor_slices((X_noised, y))\n",
    "            \n",
    "        return tf.data.Dataset.from_tensor_slices((X_noised, \n",
    "                                                    {\n",
    "                                            \"denoising\":X          \n",
    "                                            }))\n",
    "    def build_dataset(self, X, X_noised, y, shuffle, buffer_size, batch_size):  \n",
    "        dataset = self.create_dataset_from_data(X, X_noised, y)\n",
    "\n",
    "        dataset = dataset.cache()\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size, reshuffle_each_iteration=True)\n",
    "\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    def __call__(self, batch_size=256, shuffle=True, buffer_size=1000, val_prop=0.2):\n",
    "        X, X_noised, y = self.X, self.X_noised, self.y\n",
    "        dataset = self.build_dataset(X, X_noised, y, shuffle, buffer_size, batch_size)\n",
    "        train_dataset = dataset.skip(int(len(X) * val_prop))\n",
    "        val_dataset = dataset.take(int(len(X) * val_prop)) \n",
    "        \n",
    "        train_dataset = train_dataset.batch(batch_size)\n",
    "        train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        val_dataset = val_dataset.batch(batch_size)\n",
    "        val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a85165",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMNIST_MORPHO_REG():\n",
    "    def __init__(self, mode=\"classification\", noise_train={\"mean\":0, \"std\":[0]}, \n",
    "                noise_val={\"mean\":0, \"std\":[0]}, reconstruction=True,\n",
    "                noise=None, shuffle=True, name=\"\",  X=None, X_noised=None, y=None,\n",
    "                buffer_size=10000):\n",
    "        self.noise = noise\n",
    "        self.name = name\n",
    "        self.noise_train = noise_train\n",
    "        self.noise_val = noise_val\n",
    "        self.mode = mode\n",
    "        self.X = X\n",
    "        self.X_noised = X_noised\n",
    "        self.y = y\n",
    "        self.rec = reconstruction\n",
    "\n",
    "\n",
    "    def create_dataset_from_data(self, X=None, X_noised=None, y=None):\n",
    "        if self.mode == \"classification\":\n",
    "            return tf.data.Dataset.from_tensor_slices((X_noised, y))\n",
    "            \n",
    "        return tf.data.Dataset.from_tensor_slices((X_noised, {\"mse_denoising\":X,\"r_max_reg\":X}))\n",
    "    def build_dataset(self, X, X_noised, y, shuffle, buffer_size, batch_size):  \n",
    "        dataset = self.create_dataset_from_data(X, X_noised, y)\n",
    "\n",
    "        dataset = dataset.cache()\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size, reshuffle_each_iteration=True)\n",
    "\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    def __call__(self, batch_size=256, shuffle=True, buffer_size=1000, val_prop=0.2):\n",
    "        X, X_noised, y = self.X, self.X_noised, self.y\n",
    "        dataset = self.build_dataset(X, X_noised, y, shuffle, buffer_size, batch_size)\n",
    "        train_dataset = dataset.skip(int(len(X) * val_prop))\n",
    "        val_dataset = dataset.take(int(len(X) * val_prop)) \n",
    "        \n",
    "        train_dataset = train_dataset.batch(batch_size)\n",
    "        train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        val_dataset = val_dataset.batch(batch_size)\n",
    "        val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e28d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hextrema(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Hextrema, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Hextrema, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return ml.h_maxima_transform([x[0], x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cb3195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_denoising_block(tf.keras.Model):\n",
    "    def __init__(self, dropout=0.1, name=\"CNN_denoising_rec\"):       \n",
    "        super(CNN_denoising_block, self).__init__(name=name)\n",
    "        self.conv1 = kl.Conv2D(24, kernel_size=(3,3), padding=\"same\",activation='relu')\n",
    "        self.conv2 = kl.Conv2D(24, kernel_size=(3,3), padding=\"same\",activation='relu')\n",
    "        self.dense = kl.Dense(1, name=\"denoising\")\n",
    "        self.batchnorm1 = kl.BatchNormalization()\n",
    "        self.batchnorm2 = kl.BatchNormalization()\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05364d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class h_extrema_denoising_block(tf.keras.Model):\n",
    "    def __init__(self, dropout=0.1, name=\"mse_denoising\"):       \n",
    "        super(h_extrema_denoising_block, self).__init__(name=name)\n",
    "        self.conv1 = kl.Conv2D(12, kernel_size=(3,3), padding=\"valid\")\n",
    "        self.conv2 = kl.Conv2D(1, kernel_size=(3,3), padding=\"valid\")\n",
    "        self.maxpooling = kl.MaxPool2D(pool_size=(2,2), padding=\"valid\")\n",
    "        self.dropout = kl.Dropout(dropout)\n",
    "        #self.globalavgpooling = kl.GlobalAveragePooling2D(data_format='channels_last')\n",
    "        self.globalavgpooling = kl.GlobalMaxPooling2D(data_format='channels_last')\n",
    "        self.dense = kl.Dense(1,kernel_constraint=tf.keras.constraints.NonNeg(),name=\"h_denoising\")\n",
    "        self.h_extrema_transform = Hextrema()\n",
    "        self.batchnorm1 = kl.BatchNormalization()\n",
    "        self.batchnorm2 = kl.BatchNormalization()\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        x = self.conv1(input_tensor)\n",
    "        #x = self.batchnorm1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        #x = self.batchnorm2(x)\n",
    "        #x = tf.nn.relu(x)\n",
    "        #x = self.maxpooling(x)\n",
    "        x = self.globalavgpooling(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        #x = tf.nn.relu(x)\n",
    "        h = tf.expand_dims(tf.expand_dims(x,axis=-1),axis=-1)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "897b2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRegularizer(regularizers.Regularizer):\n",
    "    \n",
    "    def __init__(self, regul, regul_l1, regul_log):\n",
    "        self.strength = regul\n",
    "        self.l1 = regul_l1\n",
    "        self.log = regul_log\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return  tf.cast(self.strength * tf.reduce_mean(tf.math.square(x)) + \n",
    "                        self.l1 * tf.reduce_mean(tf.math.abs(x)) + \n",
    "                        self.log * tf.reduce_mean(tf.math.log(tf.math.abs(x) + 1)), tf.double) \n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'strength': self.strength, 'l1': self.l1, 'log': self.log}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a05cf3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class denoising_model_CNN:\n",
    "    def __init__(self, n_epoch_nn = 1, train_batch_size=128, \n",
    "                test_batch_size=256, lr_nn=1e-3, size_input=(None,None,1), \n",
    "                lamb = 0., name=\"NO NAME SPECIFIED\", noise_std=0.1,\n",
    "                train_shift=0., dropout=0.1, \n",
    "                dropout_in=0.):\n",
    "\n",
    "        # Defining Classifier Parameters\n",
    "        self.n_epoch_nn = n_epoch_nn\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.lr_nn = lr_nn\n",
    "        self.size_input = size_input\n",
    "        self.name = name\n",
    "        \n",
    "        self.denoising_block = CNN_denoising_block(name=\"denoising\", dropout=dropout)\n",
    "        print(\"Creating architecture of \", name, \"  ...\")\n",
    "        # neural network loss\n",
    "        self.loss_nn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        # --------NETWORK ARCHITECTURE ----------\n",
    "        xin=kl.Input(shape=tf.TensorShape(size_input), dtype=tf.double)\n",
    "        xin_noised = train_shift +xin + tf.cast(tf.abs(kl.GaussianNoise(noise_std)(tf.zeros_like(xin))), tf.double)\n",
    "        \n",
    "        if dropout_in >0.:\n",
    "            xin_noised = kl.ReLU()(kl.GaussianDropout(dropout_in)(xin_noised))\n",
    "        output_denoising=self.denoising_block(xin_noised)\n",
    "        #h = self.h_extrema_denoising_block(xin_noised)\n",
    "        #h = kl.ReLU(activity_regularizer=MyRegularizer(regul_l1=regul_l1, regul=regul, regul_log=regul_log))(h) \n",
    "        \n",
    "        # h_origin = self.h_extrema_denoising_block(xin)\n",
    "        # h_origin = kl.ReLU(activity_regularizer=MyRegularizer(regul_l1=regul_l1_or, regul=regul_or, regul_log=regul_log_or))(h_origin) \n",
    "        #output_h_denoising = Hextrema(name=\"h_extrema_denoising_rec\")([h, xin_noised])\n",
    "        #rmax_reg = kl.Lambda(ml.region_maxima_transform)(output_h_denoising)\n",
    "        #rmax_reg = kl.Multiply(name=\"r_max_reg\")([output_h_denoising, 255.*rmax_reg])\n",
    "\n",
    "        # Models Creation\n",
    "        self.nn_output_h_denoising = km.Model(xin, output_denoising, name=\"output_h_denoising\")\n",
    "        self.nn = km.Model(xin, output_denoising)\n",
    "        \n",
    "        #self.nn_output_h_denoising = km.Model(xin, output_h_denoising, name=\"output_h_denoising\")\n",
    "        #[output_h_denoising, rmax_reg])\n",
    "        #self.nn_r_max = km.Model(xin, rmax_reg)\n",
    "        #self.nn_output_h_denoising = km.Model(xin, output_h_denoising, name=\"output_h_denoising\")\n",
    "        #self.nn_h = km.Model(xin, h)\n",
    "\n",
    "        # Optimizer, learning rate and loss choices\n",
    "        self.nn_opt = tf.keras.optimizers.Adam(learning_rate=self.lr_nn)\n",
    "        self.nn.compile(optimizer=self.nn_opt, \n",
    "                        loss=self.loss_nn)\n",
    "        print(\"Done\\n\")\n",
    "\n",
    "\n",
    "    #def predict_h(self, X):\n",
    "    #    return tf.convert_to_tensor(self.nn_h.predict(X), tf.double)\n",
    "    \n",
    "    #def predict_H(self, X):\n",
    "    #    return tf.convert_to_tensor(self.nn_H.predict(X), tf.double)\n",
    "\n",
    "    #def predict_r_max(self, X):\n",
    "    #    return tf.convert_to_tensor(self.nn_r_max.predict(X), tf.double)\n",
    "\n",
    "    def predict_output_h_denoising(self, X):\n",
    "        return tf.convert_to_tensor(self.nn.predict(X), \n",
    "                                    tf.double)\n",
    "    #def predict_output_H_denoising(self, X):\n",
    "    #    return tf.convert_to_tensor(self.nn_out_H.predict(X), \n",
    "    #                                tf.double)\n",
    "\n",
    "    def train(self, train_dataset, val_dataset, verbose=1):\n",
    "        if verbose == 1:\n",
    "            self.nn.summary()\n",
    "\n",
    "        #Callback definition\n",
    "        CBs=[EarlyStopping(monitor='val_loss', patience=30,\n",
    "                        restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                factor=0.1,patience=10,\n",
    "                                                min_lr=0.00001)]\n",
    "        #Training the model\n",
    "        self.historyi = self.nn.fit(x=train_dataset, \n",
    "                                    validation_data=val_dataset, \n",
    "                                    epochs=self.n_epoch_nn)#, callbacks=[CBs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01641f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class denoising_model:\n",
    "    def __init__(self, n_epoch_nn = 1, train_batch_size=128, \n",
    "                test_batch_size=256, lr_nn=1e-3, size_input=(28,28,1),#size_input=(None,None,1), \n",
    "                lamb = 0., name=\"NO NAME SPECIFIED\", noise_std=0.1,\n",
    "                reconstruction_steps=-1, hext_coeff=1., rmax_reg_coeff=1.,\n",
    "                train_shift=0., regul = 0.1, regul_l1=0., regul_log=0.1, regul_or = 0.1, regul_l1_or=0., regul_log_or=0.1, dropout=0.1, \n",
    "                dropout_in=0.):\n",
    "\n",
    "        # Defining Classifier Parameters\n",
    "        self.n_epoch_nn = n_epoch_nn\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.lr_nn = lr_nn\n",
    "        self.size_input = size_input\n",
    "        self.name = name\n",
    "        \n",
    "        self.h_extrema_denoising_block = h_extrema_denoising_block(name=\"t\", dropout=dropout)\n",
    "\n",
    "        print(\"Creating architecture of \", name, \"  ...\")\n",
    "        # neural network loss\n",
    "        self.loss_nn = h_extrema_denoising_block_test_loss(\n",
    "            hext_coeff=hext_coeff, rmax_reg_coeff=rmax_reg_coeff, \n",
    "            train_shift = train_shift\n",
    "            )()\n",
    "        \n",
    "        #self.loss_nn=tf.keras.losses.MeanSquaredError() #,tf.keras.losses.BinaryCrossentropy()]\n",
    "\n",
    "        # --------NETWORK ARCHITECTURE ----------\n",
    "        xin=kl.Input(shape=tf.TensorShape(size_input), dtype=tf.double)\n",
    "        xin_noised = train_shift +xin + tf.cast(tf.abs(kl.GaussianNoise(noise_std)(tf.zeros_like(xin))), tf.double)\n",
    "        \n",
    "        if dropout_in >0.:\n",
    "            xin_noised = kl.ReLU()(kl.GaussianDropout(dropout_in)(xin_noised))\n",
    "        h = self.h_extrema_denoising_block(xin_noised)\n",
    "        #h = kl.ReLU(activity_regularizer=MyRegularizer(regul_l1=regul_l1, regul=regul, regul_log=regul_log))(h) \n",
    "        #h = tf.math.maxim(.01,h)\n",
    "        # h_origin = self.h_extrema_denoising_block(xin)\n",
    "        # h_origin = kl.ReLU(activity_regularizer=MyRegularizer(regul_l1=regul_l1_or, regul=regul_or, regul_log=regul_log_or))(h_origin) \n",
    "        output_h_denoising = Hextrema(name=\"mse_denoising\")([h, xin_noised])\n",
    "        rmax_reg = kl.Lambda(ml.region_maxima_transform)(output_h_denoising)\n",
    "        #rmax_reg = kl.Multiply(name=\"r_max_reg\")([output_h_denoising, rmax_reg])\n",
    "        rmax_reg = kl.Multiply(name=\"r_max_reg\")([output_h_denoising, 255.*rmax_reg])\n",
    "        #rmax_reg = kl.MaxPool2D(pool_size=(2,2), padding=\"valid\",name=\"r_max_reg\")(rmax_reg)\n",
    "        # Models Creation\n",
    "        self.nn = km.Model(xin, [output_h_denoising, rmax_reg])\n",
    "        self.nn_r_max = km.Model(xin, rmax_reg)\n",
    "        self.nn_output_h_denoising = km.Model(xin, output_h_denoising, name=\"output_h_denoising\")\n",
    "        self.nn_h = km.Model(xin, h)\n",
    "\n",
    "        # Optimizer, learning rate and loss choices\n",
    "        self.nn_opt = tf.keras.optimizers.Adam(learning_rate=self.lr_nn)\n",
    "        #self.nn.compile(optimizer=self.nn_opt,\n",
    "        #                loss={'mse_denoising': 'mse', 'r_max_reg': 'mae'},\n",
    "        #                loss_weights={'mse_denoising': 1., 'r_max_reg': 1.})\n",
    "        self.nn.compile(optimizer=self.nn_opt,\n",
    "                        loss=self.loss_nn)\n",
    "        print(\"Done\\n\")\n",
    "        self.nn.summary()\n",
    "\n",
    "    \n",
    "    def predict_h(self, X):\n",
    "        return tf.convert_to_tensor(self.nn_h.predict(X), tf.double)\n",
    "\n",
    "    \n",
    "    def predict_r_max(self, X):\n",
    "        return tf.convert_to_tensor(self.nn_r_max.predict(X), tf.double)\n",
    "\n",
    "    def predict_output_h_denoising(self, X):\n",
    "        return tf.convert_to_tensor(self.nn_output_h_denoising.predict(X), \n",
    "                                    tf.double)\n",
    "    #def predict_H(self, X):\n",
    "    #    return tf.convert_to_tensor(self.nn_H.predict(X), tf.double)\n",
    "\n",
    "\n",
    "    #def predict_output_H_denoising(self, X):\n",
    "    #    return tf.convert_to_tensor(self.nn_out_H.predict(X), \n",
    "    #                                tf.double)\n",
    "\n",
    "    def train(self, train_dataset, val_dataset, verbose=1):\n",
    "        #if verbose == 1:\n",
    "        #    self.nn.summary()\n",
    "\n",
    "        #Callback definition\n",
    "        CBs=[EarlyStopping(monitor='val_loss', patience=30,restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                factor=0.1,patience=10,\n",
    "                                                min_lr=0.00001)]\n",
    "        #Training the model\n",
    "        self.historyi = self.nn.fit(x=train_dataset, \n",
    "                                    validation_data=val_dataset, \n",
    "                                    epochs=self.n_epoch_nn, callbacks=[CBs]\n",
    "                                    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "548f433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_random_dataset_sample_rec(L, n = 1, rand=True, lim=(0,1), col_title = []):\n",
    "    \"\"\"Display a sample of size n from a dataset (X, y)\n",
    "\n",
    "    Args:\n",
    "        X (tf.Tensor): data with shape (number of data, image width, image height, channel)\n",
    "        y (tf.tensor): labels\n",
    "        n (int, optional): Sample size. Defaults to 1.\n",
    "    \"\"\"\n",
    "    expand = len(L) == 1\n",
    "    L = [tens.numpy() for tens in L]\n",
    "    len_dataset = len(L[0])\n",
    "    if rand == True:\n",
    "        p = np.random.permutation(len_dataset)\n",
    "        L = [elem[p] for elem in L]\n",
    "    nb_subplots = len(L)\n",
    "    L = np.stack(L)\n",
    "    if expand: L=np.expand_dims(L, axis=0)\n",
    "    \n",
    "    if col_title == []:  col_title=[\"\" for i in range(len(L))]\n",
    "    \n",
    "    plt.figure(figsize=(16,14))\n",
    "    for j in range(1, nb_subplots+1):\n",
    "        for i in range(n):\n",
    "            plt.subplot(n, nb_subplots, nb_subplots*i+j)\n",
    "            if L.shape[3] == 1:\n",
    "                \n",
    "                plt.plot(L[j-1, i,:,0,0])\n",
    "            else:\n",
    "                plt.axis('off')\n",
    "                if lim==(0,0):\n",
    "                    plt.imshow(L[j-1,i,:,:,:])\n",
    "                else:\n",
    "                    plt.imshow(L[j-1,i,:,:,:], clim=lim)\n",
    "            if i == 0:\n",
    "                plt.title(col_title[j-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74e83c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(classification_nn):\n",
    "    h_pred_1_denoised=[]\n",
    "    \n",
    "    print(\"------------FOLDED NOISE-----------\")\n",
    "    _, (x_test, y_test) = mnist.load_data()\n",
    "    X, y = x_test/255., y_test\n",
    "    X, y = tf.cast(tf.convert_to_tensor(X), tf.double), tf.cast(tf.convert_to_tensor(y), tf.double)\n",
    "    absi = tf.linspace(0.,1,10)\n",
    "    for i in absi:\n",
    "        X_pred, y_pred_true = tf.expand_dims(X, axis=3), tf.convert_to_tensor(tf.keras.utils.to_categorical(y))\n",
    "        X_pred = X_pred+tf.cast(tf.abs(kl.GaussianNoise(i)(tf.zeros(shape=tf.TensorShape(X_pred.shape)), training=True)), tf.double)\n",
    "        h_pred_denoised = classification_nn.predict_h(X_pred)\n",
    "        h_pred_1_denoised.append(h_pred_denoised) \n",
    "\n",
    "    hb_denoised_mnist = tf.concat(h_pred_1_denoised, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.title('h boxplot for MNIST')\n",
    "    plt.ylabel('h')\n",
    "    plt.xlabel('Folded noise deviation')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.boxplot(hb_denoised_mnist.numpy())\n",
    "    plt.xticks([0,1,2,3,4,5,6,7,8,9,], [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    h_pred_1_denoised=[]\n",
    "    \n",
    "    print(\"------------FOLDED NOISE-----------\")\n",
    "    _, (x_test, y_test) = mnist.load_data()\n",
    "    X, y = x_test/255., y_test\n",
    "    X, y = tf.cast(tf.convert_to_tensor(X), tf.double), tf.cast(tf.convert_to_tensor(y), tf.double)\n",
    "    absi = tf.linspace(0.,1,10)\n",
    "    for i in absi:\n",
    "        X_pred, y_pred_true = tf.expand_dims(X, axis=3), tf.convert_to_tensor(tf.keras.utils.to_categorical(y))\n",
    "        X_pred = X_pred+tf.multiply(X_pred, tf.cast(tf.abs(kl.GaussianNoise(i)(tf.zeros(shape=tf.TensorShape(X_pred.shape)), training=True)), tf.double))\n",
    "        h_pred_denoised = classification_nn.predict_h(X_pred)\n",
    "        h_pred_1_denoised.append(h_pred_denoised) \n",
    "\n",
    "    hb_denoised_mnist = tf.concat(h_pred_1_denoised, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.title('h boxplot for MNIST')\n",
    "    plt.ylabel('h')\n",
    "    plt.xlabel('Folded noise deviation')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.boxplot(hb_denoised_mnist.numpy())\n",
    "    plt.xticks([0,1,2,3,4,5,6,7,8,9,], [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # h_pred_1_denoised=[]\n",
    "\n",
    "    # _, (x_test, y_test) = fashion_mnist.load_data()\n",
    "    # X, y = x_test/255., y_test\n",
    "    # X, y = tf.cast(tf.convert_to_tensor(X), tf.double), tf.cast(tf.convert_to_tensor(y), tf.double)\n",
    "    # absi = tf.linspace(0.,1,10)\n",
    "    # for i in absi:\n",
    "    #     X_pred, y_pred_true = tf.expand_dims(X, axis=3), tf.convert_to_tensor(tf.keras.utils.to_categorical(y))\n",
    "    #     X_pred = X_pred+tf.cast(tf.abs(kl.GaussianNoise(i)(tf.zeros(shape=tf.TensorShape(X_pred.shape)), training=True)), tf.double)\n",
    "    #     h_pred_denoised = classification_nn.predict_h(X_pred)\n",
    "    #     h_pred_1_denoised.append(h_pred_denoised) \n",
    "\n",
    "    # hb_denoised_fashion_mnist = tf.concat(h_pred_1_denoised, axis=1)\n",
    "\n",
    "    # # plt.figure(figsize=(15, 5))\n",
    "    # # plt.title('h boxplot for Fashion MNIST')\n",
    "    # # plt.ylabel('h')\n",
    "    # # plt.xlabel('Folded noise deviation')\n",
    "    \n",
    "    # # plt.grid()\n",
    "    # # plt.boxplot(hb_denoised_fashion_mnist.numpy())\n",
    "    # # plt.xticks([0,1,2,3,4,5,6,7,8,9,10], [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.])\n",
    "    # # plt.show()\n",
    "    \n",
    "    # # (X, y), _ = mnist.load_data()\n",
    "    # _, (X, y) = datasets.cifar10.load_data()\n",
    "    # h_pred_1_denoised=[]\n",
    "    # # Normalize pixel values to be between 0 and 1\n",
    "    # X = tf.convert_to_tensor(X/255.)\n",
    "    # X = tf.cast(tf.reduce_mean(X, axis=-1), tf.double)\n",
    "    # absi = tf.linspace(0.,1,10)\n",
    "    # for i in absi:\n",
    "    #     X_pred, y_pred_true = tf.expand_dims(X, axis=3), tf.convert_to_tensor(tf.keras.utils.to_categorical(y))\n",
    "    #     X_pred = X_pred+tf.cast(tf.abs(kl.GaussianNoise(i)(tf.zeros(shape=tf.TensorShape([10000,32,32,1])), training=True)), tf.double)\n",
    "    #     h_pred_denoised = classification_nn.predict_h(X_pred)\n",
    "    #     h_pred_1_denoised.append(h_pred_denoised) \n",
    "\n",
    "    # hb_denoised_cifar10 = tf.concat(h_pred_1_denoised, axis=1)\n",
    "\n",
    "    # plt.figure(figsize=(15, 5))\n",
    "    # plt.title('h boxplot for CIFAR10')\n",
    "    # plt.ylabel('h')\n",
    "    # plt.xlabel('Folded noise deviation')\n",
    "    \n",
    "    # plt.grid()\n",
    "    # plt.boxplot(hb_denoised_cifar10.numpy())\n",
    "    # plt.xticks([0,1,2,3,4,5,6,7,8,9,10], [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.])\n",
    "    # plt.show()\n",
    "    \n",
    "    # plt.figure(figsize=(15, 5))\n",
    "    # plt.boxplot(hb_denoised_fashion_mnist.numpy())\n",
    "    # plt.boxplot(hb_denoised_mnist.numpy())\n",
    "    # plt.boxplot(hb_denoised_cifar10.numpy())\n",
    "    # plt.title('h boxplot for MNIST, Fashion MNIST and CIFAR10')\n",
    "    # plt.ylabel('h')\n",
    "    # plt.xlabel('Folded noise deviation')\n",
    "    # plt.legend(['MNIST', 'Fashion MNIST', \"CIFAR10\"], loc='upper left')\n",
    "    # plt.xticks([0,1,2,3,4,5,6,7,8,9], [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "    # plt.grid()\n",
    "    # plt.show()\n",
    "    \n",
    "    # h_pred_1_denoised=[]\n",
    "\n",
    "    # absi = tf.linspace(0.,1,10)\n",
    "    # for i in absi:\n",
    "    #     h_list = []\n",
    "    #     for image in im_data:\n",
    "    #         X_pred = image+tf.cast(tf.abs(kl.GaussianNoise(i)(tf.zeros(shape=tf.TensorShape(list(image.shape[0:3])+[1])), training=True)), tf.double)\n",
    "    #         X_pred = tf.reduce_mean(X_pred, axis=-1)\n",
    "    #         h_pred_denoised = denoising_mod.predict_h(X_pred)\n",
    "    #         h_list.append(float(h_pred_denoised))\n",
    "    #     h_pred_1_denoised.append(tf.expand_dims(tf.convert_to_tensor(h_list), axis=1)) \n",
    "\n",
    "    # hb_denoised_91image = tf.concat(h_pred_1_denoised, axis=1)\n",
    "\n",
    "    # plt.figure(figsize=(15, 5))\n",
    "    # plt.title('h boxplot for 91image')\n",
    "    # plt.ylabel('h')\n",
    "    # plt.xlabel('Folded noise deviation')\n",
    "\n",
    "    # plt.grid()\n",
    "    # plt.boxplot(hb_denoised_91image.numpy())\n",
    "    # plt.xticks([0,1,2,3,4,5,6,7,8,9,10], [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.])\n",
    "    # plt.show()\n",
    "    \n",
    "    # h_pred_1_denoised=[]\n",
    "\n",
    "    # absi = tf.linspace(0.,1,10)\n",
    "    # for i in absi:\n",
    "    #     h_list = []\n",
    "    #     for image in im_data2:\n",
    "    #         X_pred = image+tf.cast(tf.abs(kl.GaussianNoise(i)(tf.zeros(shape=tf.TensorShape(list(image.shape[0:3])+[1])), training=True)), tf.double)\n",
    "    #         X_pred = tf.reduce_mean(X_pred, axis=-1)\n",
    "    #         h_pred_denoised = denoising_mod.predict_h(X_pred)\n",
    "    #         h_list.append(float(h_pred_denoised))\n",
    "    #     h_pred_1_denoised.append(tf.expand_dims(tf.convert_to_tensor(h_list), axis=1)) \n",
    "\n",
    "    # hb_denoised_bsd300 = tf.concat(h_pred_1_denoised, axis=1)\n",
    "\n",
    "    # plt.figure(figsize=(15, 5))\n",
    "    # plt.title('h boxplot for BSD300')\n",
    "    # plt.ylabel('h')\n",
    "    # plt.xlabel('Folded noise deviation')\n",
    "\n",
    "    # plt.grid()\n",
    "    # plt.boxplot(hb_denoised_bsd300.numpy())\n",
    "    # plt.xticks([0,1,2,3,4,5,6,7,8,9,10], [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.])\n",
    "    # plt.show()\n",
    "    \n",
    "    # m, f, c, q, q2 = hb_denoised_mnist.numpy(), hb_denoised_fashion_mnist.numpy(), hb_denoised_cifar10.numpy(), hb_denoised_91image.numpy(), hb_denoised_bsd300.numpy()\n",
    "    # l = [0.0]*10000 + [0.1]*10000 + [0.2]*10000 + [0.3]*10000 + [0.4]*10000 + [0.5]*10000 + [0.6]*10000 + [0.7]*10000 + [0.8]*10000 + [0.9]*10000\n",
    "    # l = np.array(l)\n",
    "    # l2 = [0.0]*91 + [0.1]*91 + [0.2]*91 + [0.3]*91 + [0.4]*91 + [0.5]*91 + [0.6]*91 + [0.7]*91 + [0.8]*91 + [0.9]*91\n",
    "    # l3 = [0.0]*300 + [0.1]*300 + [0.2]*300 + [0.3]*300 + [0.4]*300 + [0.5]*300 + [0.6]*300 + [0.7]*300 + [0.8]*300 + [0.9]*300\n",
    "    # l2 = np.array(l2)\n",
    "    # l3 = np.array(l3)\n",
    "    # tot = np.concatenate([m.T.flatten(), f.T.flatten(), c.T.flatten(), q.T.flatten(), q2.T.flatten()], axis=0)\n",
    "    # l = np.concatenate([l, l, l, l2, l3], axis=0)\n",
    "    # tot = np.concatenate([np.expand_dims(tot, axis=1), np.expand_dims(l, axis=1)], axis=1)\n",
    "    # lab = [\"MNSIT\"]*100000 + [\"Fashion MNIST\"]*100000 + [\"CIFAR\"]*100000 + [\"91image\"]*910 + [\"BSD300\"]*3000\n",
    "    # lab = np.array(lab)\n",
    "    # tot = np.concatenate([tot, np.expand_dims(lab, axis=1)], axis=1)\n",
    "    # tot = pd.DataFrame(tot, columns=['h', 'deviation', \"dataset\"])\n",
    "    # tot = tot.astype({'deviation': 'float32', 'h': 'float32'})\n",
    "    # fig_dims = (15, 9.5)\n",
    "    # fig, ax = plt.subplots(figsize=fig_dims)\n",
    "    # plt.plot([1.7, 1.7], [0, 2.3], '--', lw=2, label=\"Training\")\n",
    "    # plt.legend(['Training'])\n",
    "    # sns.boxplot(x=\"deviation\", y=\"h\",\n",
    "    #             data=tot, hue=\"dataset\", palette=[\"m\", \"g\", \"r\", 'orange', \"blue\"], ax=ax)\n",
    "    # plt.show()\n",
    "    \n",
    "    # return hb_denoised_mnist.numpy(), hb_denoised_fashion_mnist.numpy(), hb_denoised_cifar10.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b21ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_cifar, y_cifar), (X_test_cifar, y_test_cifar) = cifar10.load_data()\n",
    "(X_mnist, y_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()\n",
    "(X_fashion, y_fashion), (X_test_fashion, y_test_fashion) = fashion_mnist.load_data()\n",
    "\n",
    "X = {}\n",
    "X[\"cifar\"] = {\"train\": X_cifar, \"test\": X_test_cifar}\n",
    "X[\"mnist\"] = {\"train\": X_mnist, \"test\": X_test_mnist}\n",
    "X[\"fashion\"] = {\"train\": X_fashion, \"test\": X_test_fashion}\n",
    "\n",
    "y = {\"cifar\": {\"train\": y_cifar, \"test\": y_test_cifar},\n",
    "     \"mnist\": {\"train\": y_mnist, \"test\": y_test_mnist},\n",
    "     \"fashion\": {\"train\": y_fashion, \"test\": y_test_fashion}\n",
    "}\n",
    "\n",
    "for ds in X:\n",
    "    for dataset in X[ds]:\n",
    "        X[ds][dataset] = tf.convert_to_tensor(X[ds][dataset]/255.)\n",
    "        y[ds][dataset] = tf.convert_to_tensor(y[ds][dataset])\n",
    "        if ds != \"cifar\":\n",
    "            X[ds][dataset] = tf.expand_dims(X[ds][dataset], axis=-1)\n",
    "        y[ds][dataset] = tf.keras.utils.to_categorical(y[ds][dataset])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e5a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcac6fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batcher  = DatasetMNIST(mode=\"\", reconstruction=True, \n",
    "                        name=\"no_noise\", X=X[\"mnist\"][\"train\"], X_noised=X[\"mnist\"][\"train\"], y=y[\"mnist\"][\"train\"])\n",
    "train_dataset, val_dataset = Batcher(batch_size=512)\n",
    "#  hext_coeff=300, train_shift=0.001, regul=0.0, regul_l1 = 0.000, regul_log=0.00,\n",
    "#                                     rmax_reg_coeff = 130., noise_std=0.1, dropout=0.2, dropout_in=0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9c839c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#denoising_mod_CNN = denoising_model_CNN(n_epoch_nn = 20, lr_nn=0.001, train_shift=0.00, noise_std=0.2, dropout=0.3, dropout_in=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4707822d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating architecture of  NO NAME SPECIFIED   ...\n",
      "Done\n",
      "\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.zeros_like_3 (TFOpLambda)    (None, None, None, 1 0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_3 (GaussianNoise (None, None, None, 1 0           tf.zeros_like_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs_3 (TFOpLambda)      (None, None, None, 1 0           gaussian_noise_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (None, None, None, 1 0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast_3 (TFOpLambda)          (None, None, None, 1 0           tf.math.abs_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (None, None, None, 1 0           tf.__operators__.add_6[0][0]     \n",
      "                                                                 tf.cast_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "denoising (CNN_denoising_block) (None, None, None, 1 5665        tf.__operators__.add_7[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 5,665\n",
      "Trainable params: 5,569\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:756 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:182 __call__\n        y_true = self._conform_to_outputs(y_pred, y_true)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:62 _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:587 map_to_output_names\n        struct.keys(), output_names))\n\n    ValueError: Found unexpected keys that do not correspond to any Model output: dict_keys(['mse_denoising', 'r_max_reg']). Expected: ['denoising']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e934ef35a866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdenoising_mod_CNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdenoising_model_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_nn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_shift\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdenoising_mod_CNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#del Batcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#del train_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-e915549a8560>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, verbose)\u001b[0m\n\u001b[1;32m     81\u001b[0m         self.historyi = self.nn.fit(x=train_dataset, \n\u001b[1;32m     82\u001b[0m                                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                                     epochs=self.n_epoch_nn)#, callbacks=[CBs])\n\u001b[0m",
      "\u001b[0;32m/usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:756 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:182 __call__\n        y_true = self._conform_to_outputs(y_pred, y_true)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:62 _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    /usr/local/envs/anaconda3/envs/tf-24/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:587 map_to_output_names\n        struct.keys(), output_names))\n\n    ValueError: Found unexpected keys that do not correspond to any Model output: dict_keys(['mse_denoising', 'r_max_reg']). Expected: ['denoising']\n"
     ]
    }
   ],
   "source": [
    "denoising_mod_CNN = denoising_model_CNN(n_epoch_nn = 200, lr_nn=0.001, train_shift=0.00, noise_std=0.2, dropout=0.3, dropout_in=0.,)\n",
    "denoising_mod_CNN.train(train_dataset, val_dataset)\n",
    "\n",
    "#del Batcher\n",
    "#del train_dataset\n",
    "#del val_dataset\n",
    "\n",
    "plt.plot(denoising_mod_CNN.historyi.history['loss'])\n",
    "plt.plot(denoising_mod_CNN.historyi.history['val_loss'])\n",
    "plt.title('model total loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0092c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating architecture of  NO NAME SPECIFIED   ...\n",
      "Done\n",
      "\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.zeros_like_5 (TFOpLambda)    (None, 28, 28, 1)    0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_5 (GaussianNoise (None, 28, 28, 1)    0           tf.zeros_like_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs_5 (TFOpLambda)      (None, 28, 28, 1)    0           gaussian_noise_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_10 (TFOpLa (None, 28, 28, 1)    0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast_5 (TFOpLambda)          (None, 28, 28, 1)    0           tf.math.abs_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_11 (TFOpLa (None, 28, 28, 1)    0           tf.__operators__.add_10[0][0]    \n",
      "                                                                 tf.cast_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "t (h_extrema_denoising_block)   (None, 1, 1, 1)      231         tf.__operators__.add_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mse_denoising (Hextrema)        (None, 28, 28, 1)    0           t[0][0]                          \n",
      "                                                                 tf.__operators__.add_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 28, 28, 1)    0           mse_denoising[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_2 (TFOpLambda) (None, 28, 28, 1)    0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "r_max_reg (Multiply)            (None, 28, 28, 1)    0           mse_denoising[0][0]              \n",
      "                                                                 tf.math.multiply_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 231\n",
      "Trainable params: 231\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "586/750 [======================>.......] - ETA: 12s - loss: 0.0715 - mse_denoising_loss: 0.0287 - r_max_reg_loss: 0.0428"
     ]
    }
   ],
   "source": [
    "Batcher  = DatasetMNIST_MORPHO_REG(mode=\"\", reconstruction=True, \n",
    "                        name=\"no_noise\", X=X[\"mnist\"][\"train\"], X_noised=X[\"mnist\"][\"train\"], y=y[\"mnist\"][\"train\"])\n",
    "train_dataset, val_dataset = Batcher(batch_size=64)\n",
    "#  hext_coeff=300, train_shift=0.001, regul=0.0, regul_l1 = 0.000, regul_log=0.00,\n",
    "#                                     rmax_reg_coeff = 130., noise_std=0.1, dropout=0.2, dropout_in=0.\n",
    "\n",
    "\n",
    "denoising_mod = denoising_model(n_epoch_nn = 200, lr_nn=0.001,\n",
    "                                 hext_coeff=1., train_shift=0.00, regul=0., regul_l1 = 0., regul_log=0.000,\n",
    "                                 rmax_reg_coeff = 1., noise_std=0.2, dropout=0.3, dropout_in=0.,\n",
    "                                 regul_or = 0.00, regul_l1_or=0, regul_log_or=0)\n",
    "#print('denoising_mod.nn.summary()')\n",
    "#denoising_mod.nn.summary()\n",
    "\n",
    "denoising_mod.train(train_dataset, val_dataset)\n",
    "del Batcher\n",
    "del train_dataset\n",
    "del val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec0e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "hest=denoising_mod.predict_h(X['mnist']['train'][0:1000])\n",
    "print(hest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hest[:,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a157939",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hest=denoising_mod.predict_output_h_denoising(X['mnist']['train'][0:1000])\n",
    "print(Hest.shape)\n",
    "\n",
    "res=tf.reduce_mean(tf.math.squared_difference(Hest,X['mnist']['train'][0:1000]))\n",
    "print(res.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rest=denoising_mod.predict_r_max(X['mnist']['train'][0:1000])\n",
    "print(Rest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ef765",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=234\n",
    "Z=X['mnist']['train'][idx][:,:,0]\n",
    "plt.imshow(Z)\n",
    "plt.show()\n",
    "plt.imshow(np.concatenate([Z,Hest[idx][:,:,0]],axis=-1))\n",
    "plt.show()\n",
    "print(tf.reduce_max(Hest[idx][:,:,0]))\n",
    "print(tf.reduce_min(Hest[idx][:,:,0]))\n",
    "\n",
    "plt.imshow(np.concatenate([Z,Rest[idx][:,:,0]],axis=-1))\n",
    "plt.show()\n",
    "print(tf.reduce_max(Rest[idx][:,:,0]))\n",
    "print(tf.reduce_min(Rest[idx][:,:,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d108274",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(denoising_mod.historyi.history['loss'])\n",
    "plt.plot(denoising_mod.historyi.history['val_loss'])\n",
    "plt.title('model total loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(denoising_mod.historyi.history['r_max_reg_loss'])\n",
    "plt.plot(denoising_mod.historyi.history['val_r_max_reg_loss'])\n",
    "plt.title('model region maxima loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(denoising_mod.historyi.history['mse_denoising_loss'])\n",
    "plt.plot(denoising_mod.historyi.history['val_mse_denoising_loss'])\n",
    "plt.title('model mse denoising loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gray()\n",
    "for database in ['fashion','mnist']:\n",
    "    print(database)\n",
    "    \n",
    "    ##################################################\n",
    "    ## MODEL 0: CNN DENOISING without AUGMENTATION\n",
    "    denoising_mod_CNN.denoising_block.trainable=False\n",
    "    loss =tf.losses.CategoricalCrossentropy()\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    xin=kl.Input(shape=tf.TensorShape((28,28,1)), dtype=tf.double)\n",
    "    xin_noised = xin\n",
    "    denoise = denoising_mod_CNN.nn_output_h_denoising(xin_noised)\n",
    "    x = kl.Conv2D(32, kernel_size=(3,3), activation='relu')(denoise)\n",
    "    x = kl.Conv2D(32, kernel_size=(3,3), activation='relu')(x)\n",
    "    x = kl.MaxPooling2D(pool_size=(3,3))(x)\n",
    "    x = kl.Dropout(0.5)(x)\n",
    "    x = kl.Flatten()(x)\n",
    "    output = kl.Dense(10,activation=\"softmax\", name='classification')(x)\n",
    "    model_CNN_CNN = tf.keras.Model(xin, output)\n",
    "    model_CNN_CNN.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "    hist_CNN_CNN = model_CNN_CNN.fit(x=X[database]['train'], y=y[database]['train'], batch_size=256, validation_split=0.2, epochs=50)\n",
    "\n",
    "    \n",
    "    ##################################################\n",
    "    ## MODEL 0: CNN DENOISING with AUGMENTATION\n",
    "    denoising_mod_CNN.denoising_block.trainable=False\n",
    "    loss =tf.losses.CategoricalCrossentropy()\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    xin=kl.Input(shape=tf.TensorShape((28,28,1)), dtype=tf.double)\n",
    "    #xin_noised = 0.1 + kl.GaussianNoise(0.1)(xin)\n",
    "    xin_noised = kl.GaussianNoise(0.1)(xin)\n",
    "    denoise = denoising_mod_CNN.nn_output_h_denoising(xin_noised)\n",
    "    x = kl.Conv2D(32, kernel_size=(3,3), activation='relu')(denoise)\n",
    "    x = kl.Conv2D(32, kernel_size=(3,3), activation='relu')(x)\n",
    "    x = kl.MaxPooling2D(pool_size=(3,3))(x)\n",
    "    x = kl.Dropout(0.5)(x)\n",
    "    x = kl.Flatten()(x)\n",
    "    output = kl.Dense(10,activation=\"softmax\", name='classification')(x)\n",
    "    model_CNN_CNN_NOISE = tf.keras.Model(xin, output)\n",
    "    model_CNN_CNN_NOISE.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "    hist_CNN_CNN_NOISE = model_CNN_CNN_NOISE.fit(x=X[database]['train'], y=y[database]['train'], batch_size=256, validation_split=0.2, epochs=50)\n",
    " \n",
    "\n",
    "    \n",
    "    \n",
    "    ##################################################\n",
    "    ## MODEL 1: PROPOSED without AUGMENTATION\n",
    "    denoising_mod.nn_output_h_denoising.trainable=False\n",
    "    loss =tf.losses.CategoricalCrossentropy()\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    xin=kl.Input(shape=tf.TensorShape((28,28,1)), dtype=tf.double)\n",
    "    xin_noised = xin\n",
    "    denoise = denoising_mod.nn_output_h_denoising(xin_noised)\n",
    "    x = kl.Conv2D(32, kernel_size=(3,3), activation='relu')(denoise)\n",
    "    x = kl.Conv2D(32, kernel_size=(3,3), activation='relu')(x)\n",
    "    x = kl.MaxPooling2D(pool_size=(3,3))(x)\n",
    "    x = kl.Dropout(0.5)(x)\n",
    "    x = kl.Flatten()(x)\n",
    "    output = kl.Dense(10,activation=\"softmax\", name='classification')(x)\n",
    "    model_H = tf.keras.Model(xin, output)\n",
    "    model_H.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "    hist = model_H.fit(x=X[database]['train'], y=y[database]['train'], batch_size=256, validation_split=0.2, epochs=50)\n",
    "    \n",
    "    ##################################################\n",
    "    ##MODEL 2: PROPOSED with AUGMENTATION\n",
    "    \n",
    "    denoising_mod.nn_output_h_denoising.trainable=False\n",
    "    loss =tf.losses.CategoricalCrossentropy()\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    xin=kl.Input(shape=tf.TensorShape((28,28,1)), dtype=tf.double)\n",
    "    #xin_noised = 0.1 + kl.GaussianNoise(0.1)(xin)\n",
    "    xin_noised = kl.GaussianNoise(0.1)(xin)\n",
    "    denoise = denoising_mod.nn_output_h_denoising(xin_noised)\n",
    "    x = kl.Conv2D(32, kernel_size=(3,3), activation='relu')(denoise)\n",
    "    x = kl.Conv2D(32, kernel_size=(3,3), activation='relu')(x)\n",
    "    x = kl.MaxPooling2D(pool_size=(3,3))(x)\n",
    "    x = kl.Dropout(0.5)(x)\n",
    "    x = kl.Flatten()(x)\n",
    "    output = kl.Dense(10,activation=\"softmax\", name='classification')(x)\n",
    "    model_H_NOISE = tf.keras.Model(xin, output)\n",
    "    model_H_NOISE.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "    hist_NOISE = model_H_NOISE.fit(x=X[database]['train'], y=y[database]['train'], batch_size=256, validation_split=0.2, epochs=50)\n",
    "\n",
    "    ##################################################\n",
    "    ##MODEL 3: CLASSICAL CNN without augmentation\n",
    "    xin=kl.Input(shape=tf.TensorShape((28,28,1)), dtype=tf.double)\n",
    "    xin_noised = xin\n",
    "    x = kl.Conv2D(32, kernel_size=(3,3), activation='relu')(xin_noised)\n",
    "    x = kl.Conv2D(32, kernel_size=(3,3), activation='relu')(x)\n",
    "    x = kl.MaxPooling2D(pool_size=(3,3))(x)\n",
    "    x = kl.Dropout(0.5)(x)\n",
    "    x = kl.Flatten()(x)\n",
    "    output = kl.Dense(10,activation=\"softmax\", name='classification')(x)\n",
    "    model_CNN = tf.keras.Model(xin, output)\n",
    "    model_CNN.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "    hist_CNN = model_CNN.fit(x=X[database]['train'], y=y[database]['train'], batch_size=256, validation_split=0.2, epochs=50)\n",
    "\n",
    "    ##################################################\n",
    "    ##MODEL 4: CLASSICAL CNN with augmentation\n",
    "    xin=kl.Input(shape=tf.TensorShape((28,28,1)), dtype=tf.double)\n",
    "    xin_noised = kl.GaussianNoise(0.1)(xin)\n",
    "    #xin_noised = 0.1 + kl.GaussianNoise(0.1)(xin)\n",
    "    x = kl.Conv2D(32, kernel_size=(3,3), activation='relu')(xin_noised)\n",
    "    x = kl.Conv2D(32, kernel_size=(3,3), activation='relu')(x)\n",
    "    x = kl.MaxPooling2D(pool_size=(3,3))(x)\n",
    "    x = kl.Dropout(0.5)(x)\n",
    "    x = kl.Flatten()(x)\n",
    "    output = kl.Dense(10,activation=\"softmax\", name='classification')(x)\n",
    "    model_CNN_NOISE = tf.keras.Model(xin, output)\n",
    "    model_CNN_NOISE.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "    hist_CNN_NOISE = model_CNN_NOISE.fit(x=X[database]['train'], y=y[database]['train'], batch_size=256, validation_split=0.2, epochs=50)\n",
    "\n",
    "    print('Staring comparison on noise images')\n",
    "    for noise_type in ['uniform','normal']:\n",
    "        accCNN_CNN=[]\n",
    "        accCNN_CNN_NOISE=[]\n",
    "        accCNN=[]\n",
    "        accCNN_NOISE=[]\n",
    "        accH=[]\n",
    "        accH_NOISE=[]    \n",
    "        for i in tf.linspace(0,1,20):\n",
    "            i = tf.cast(i, tf.float32)\n",
    "            # X_pred = X[\"mnist\"][\"test\"]+tf.multiply(X[\"mnist\"][\"test\"], tf.cast(tf.abs(kl.GaussianNoise(i)(tf.zeros(shape=tf.TensorShape(X[\"mnist\"][\"test\"].shape)), training=True)), tf.double))\n",
    "            if noise_type=='uniform':\n",
    "                X_pred = X[database][\"test\"] + tf.cast(tf.random.uniform(X[database][\"test\"].shape, 0., i), tf .double)\n",
    "            elif noise_type=='normal':\n",
    "                X_pred = X[database][\"test\"] + tf.cast(tf.random.normal(X[database][\"test\"].shape, 0., i), tf .double)\n",
    "            plt.imshow(X_pred[0,:,:,0])\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            accCNN_CNN.append(model_CNN_CNN.evaluate(X_pred, y[database][\"test\"], batch_size=512, verbose=0)[1])\n",
    "            accCNN_CNN_NOISE.append(model_CNN_CNN_NOISE.evaluate(X_pred, y[database][\"test\"], batch_size=512, verbose=0)[1])\n",
    "            accCNN.append(model_CNN.evaluate(X_pred, y[database][\"test\"], batch_size=512, verbose=0)[1])\n",
    "            accCNN_NOISE.append(model_CNN_NOISE.evaluate(X_pred, y[database][\"test\"], batch_size=512, verbose=0)[1])\n",
    "            accH.append(model_H.evaluate(X_pred, y[database][\"test\"], batch_size=512, verbose=0)[1])\n",
    "            accH_NOISE.append(model_H_NOISE.evaluate(X_pred, y[database][\"test\"], batch_size=512, verbose=0)[1])\n",
    "        np.save(noise_type+database+'accCNN_CNN',accCNN_CNN)\n",
    "        np.save(noise_type+database+'accCNN_CNN_NOISE',accCNN_CNN_NOISE)\n",
    "        np.save(noise_type+database+'accCNN',accCNN)\n",
    "        np.save(noise_type+database+'accCNN_NOISE',accCNN_NOISE)\n",
    "        np.save(noise_type+database+'accCNNH',accH)\n",
    "        np.save(noise_type+database+'accCNNH_NOISE',accH_NOISE)\n",
    "    #plt.savefig('FigsAyoub/'+database+'_COMPARISON_RESULT.pgf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "for save_fig in [False,True]:\n",
    "    if save_fig:\n",
    "        import matplotlib\n",
    "        matplotlib.use(\"pgf\")\n",
    "        matplotlib.rcParams.update({\n",
    "            \"pgf.texsystem\": \"pdflatex\",\n",
    "            'font.family': 'serif',\n",
    "            'text.usetex': True,\n",
    "            'pgf.rcfonts': False,\n",
    "        })\n",
    "    database_list=['mnist','fashion']\n",
    "    database_name=['MNIST','FASHION MNIST']\n",
    "    noise_type=['uniform','normal']\n",
    "    for noisename in noise_type:\n",
    "        for database,databasename in zip(database_list,database_name):\n",
    "            accCNN_CNN=np.load(noisename+database+'accCNN_CNN.npy')\n",
    "            accCNN_CNN_NOISE=np.load(noisename+database+'accCNN_CNN_NOISE.npy')\n",
    "            accCNN=np.load(noisename+database+'accCNN.npy')\n",
    "            accCNN_NOISE=np.load(noisename+database+'accCNN_NOISE.npy')\n",
    "            accH=np.load(noisename+database+'accCNNH.npy')\n",
    "            accH_NOISE=np.load(noisename+database+'accCNNH_NOISE.npy')\n",
    "            plt.figure()\n",
    "            plt.plot(np.linspace(0,1,20),accCNN,label='CNN')\n",
    "            plt.plot(np.linspace(0,1,20),accCNN_NOISE,label='CNN Augmentation')\n",
    "            plt.plot(np.linspace(0,1,20),accCNN_CNN,label='CNN_CNN')\n",
    "            plt.plot(np.linspace(0,1,20),accCNN_CNN_NOISE,label='CNN_CNN Augmentation')\n",
    "            plt.plot(np.linspace(0,1,20),accH,label='HMAX CNN')\n",
    "            plt.plot(np.linspace(0,1,20),accH_NOISE,label='HMAX CNN Augmentation')\n",
    "            plt.xlim([0,1])\n",
    "            if save_fig:\n",
    "                print('.')\n",
    "            else:\n",
    "                plt.title(databasename+' '+noisename+' noise')\n",
    "            plt.xlabel('Noise Level')\n",
    "            plt.ylabel('Accuracy in Test')\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            if save_fig:\n",
    "                plt.savefig('FigsEMD/Ayoub'+database+noisename+'.pgf')\n",
    "            else:\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57949d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
